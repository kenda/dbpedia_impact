
@inproceedings{
Title = {The Semantic Gap of Formalized Meaning},
Booktitle = {7th Extended Semantic Web Conference (ESWC2010)},
Year = {2010},
Month = {June},
Abstract = {Recent work in Ontology Learning and Textmining mainly focused on engineering methods to solve practical problem. In this thesis, we investigate methods that can substantially improve a wide range of existing approaches by minimizing the underlying problem: The Semantic Gap between formalized meaning and human cognition. We deploy OWL as a Meaning Representation Language and create a unified model, which combines existing NLP methods with Linguistic knowledge and aggregates disambiguated background knowledge from the Web of Data. The presented methodology allows to study and evaluate the capabilities of such aggregated knowledge to improve the efficiency of methods in Ontology Learning.},
Url = {http://data.semanticweb.org/conference/eswc/2010/paper/phd_symposium/30}
}
@inproceedings{
Author = {Alexandre Passant},
Title = {dbrec - Music Recommendations Using DBpedia},
Booktitle = {9th International Semantic Web Conference (ISWC2010)},
Year = {2010},
Month = {November},
Abstract = {This paper describes the theoretical background and the implementation of dbrec, a music recommendation system built on top of DBpedia, offering recommendations for more than 39,000 bands and solo artists. We discuss the various challenges and lessons learnt while building it, providing relevant insights for people developing applications consuming Linked Data. Furthermore, we provide a user-centric evaluation of the system, notably by comparing it to last.fm.},
Url = {http://data.semanticweb.org/conference/iswc/2010/paper/431}
}
@inproceedings{
Author = {Mihaly Heder and Pablo Mendes},
Title = {Round-trip semantics with Sztakipedia and DBpedia Spotlight},
Booktitle = {21st International World Wide Web Conference (WWW2012)},
Year = {2012},
Abstract = {We describe a tool kit to support a knowledge-enhancement
cycle on the Web. In the first step, structured data which
is extracted from Wikipedia is used to construct automatic
content enhancement engines. Those engines can be used
to interconnect knowledge in structured and unstructured
information sources on the Web, including Wikipedia it-
self. Sztakipedia-toolbar is a MediaWiki user script which
brings DBpedia Spotlight and other kinds of machine in-
telligence into the Wiki editor interface to provide enhance-
ment suggestions to the user. The suggestions offered by the
tool focus on complementing knowledge and increasing the
availability of structured data on Wikipedia. This will, in
turn, increase the available information for the content en-
hancement engines themselves, completing a virtuous cycle
of knowledge enhancement.},
Url = {http://data.semanticweb.org/conference/www/2012/demo/62}
}
@inproceedings{
Author = {Andrea Nuzzolese and Aldo Gangemi and Valentina Presutti and Paolo Ciancarini},
Title = {Type inference through the analysis of Wikipedia links},
Booktitle = {LDOW 2012 : linked Data on the web (LDOW2012)},
Year = {2012},
Abstract = {DBpedia contains millions of untyped entities, either if we consider the native DBpedia ontology, or Yago plus WordNet. Is it possible to automatically classify those entities? Based on previous work on wikilink invariances, we wondered if wikilinks convey a knowledge rich enough for their classification.In this paper we give three contributions. Concerning the DBpedia link structure, we describe some measurements and notice both problems (e.g. the bias that could be induced by the incomplete ontological coverage of the DBpedia ontology), and potentials existing in current type coverage. Concerning classification, we present two techniques that exploit wikilinks, one based on induction from machine learning techniques, and the other on abduction. Finally, we discuss the limited results of classification, which confirmed our fears expressed in the description of general figures from the measurement. We also suggest some new possible approaches to entity classification that could be taken, based on more solid grounds.},
Url = {http://data.semanticweb.org/workshop/ldow/2012/paper/30}
}
@inproceedings{
Author = {Diego Torres and Pascal Molli and Hala Skaf-Molli and Alicia Díaz},
Title = {Improving Wikipedia with DBpedia},
Booktitle = {SWCS: Semantic Web Collaborative Spaces (SWCS 2012)},
Year = {2012},
Abstract = {DBpedia is the semantic mirror of Wikipedia. DBpedia extracts information from Wikipedia and stores it in a semantic knowledge base. This semantic feature relies in making complex queries inferring relations among articles which sometimes are missing in Wikipedia. This difference generates an information gap between DBpedia and Wikipedia. Could be improved Wikipedia with DBpedia new information to reduce this gap? How this new information should be added to Wikipedia? In this article, we propose a path indexing algorithm (PIA) who takes a data set of a DBPedia query and returns the best representative path to be applied in the Wikipedia.  We evaluate the
results of applying PIA to express the relation between people and their birth city.},
Url = {http://data.semanticweb.org/workshop/swcs/2012/paper/11}
}
@inproceedings{
Title = {DBpedia: A Nucleus for a Web of Open Data},
Booktitle = {6th International and 2nd Asian Semantic Web Conference (ISWC2007+ASWC2007)},
Year = {2007},
Month = {November},
Pages = {715--728},
Abstract = {DBpedia is a community effort to extract structured information from Wikipedia and to make this information available on the Web. DBpedia allows you to ask sophisticated queries against datasets derived from Wikipedia and to link other datasets on the Web to Wikipedia data. We describe the extraction of the DBpedia datasets, and how the resulting information can be made available on the Web for humans and machines. We describe some emerging applications from the DBpedia community and show how website operators can reduce costs by facilitating royalty-free DBpedia content within their sites. Finally, we present the current status of interlinking DBpedia with other open datasets on the Web and outline how DBpedia could serve as a nucleus for an emerging Web of open data sources.},
Url = {http://data.semanticweb.org/conference/iswc-aswc/2007/tracks/in-use/papers/715}
}
@inproceedings{
Title = {:me owl:sameAs flickr:33669349@N00 .},
Booktitle = {Linked Data on the Web (LDOW2008)},
Abstract = {In order to release the Social Semantic Web and solve data-portability issues, there is a need from Web 2.0 providers to open their data and deliver it in machine-redeable way. Modeling it with vocabularies as FOAF, SIOC, as well as reusing data from the Linked Data initiative as DBpedia or geonames.org can help to achieve this task.The goal of this demo is to showcase a RDF exporter for Flickr profiles that acts such a way.},
Url = {http://data.semanticweb.org/workshop/LDOW/2008/paper/16}
}
@inproceedings{
Title = {URI Disambiguation in the Context of Linked Data},
Booktitle = {Linked Data on the Web (LDOW2008)},
Abstract = {The Linked Data initiative has given rise to an increasing number of RDF datasets, many of which are freely accessible online. These resources often arise as a result of database exports; however sufficient consideration may not be given to the unseen implications caused when they are used in the wider context of the Semantic Web. This paper investigates two popular resources, DBLP and DBpedia, and discusses whether the issues regarding identity management and co-reference resolution have been suitably addressed. We find that a large percentage of authors in DBLP have been conflated, and that disambiguation pages have been incorrectly linked using owl:sameAs within DBpedia. Systems for dealing with these issues are presented, and directions are given for future research.},
Url = {http://data.semanticweb.org/workshop/LDOW/2008/paper/5}
}
@inproceedings{
Title = {DBpedia Mobile: A Location-Enabled Linked Data Browser},
Booktitle = {Linked Data on the Web (LDOW2008)},
Abstract = {In this demonstration, we present DBpedia Mobile, alocation-centric DBpedia client application for mobile devicesconsisting of a map view and a Fresnel-based LinkedData browser. The DBpedia project extracts structuredinformation from Wikipedia and publishes this informationas Linked Data on the Web. The DBpedia dataset containsinformation about 1.95 million things, including 200,000geographic locations. DBpedia is interlinked with variousother location-related datasets. Based on the current GPSposition of a mobile device, DBpedia Mobile renders amap indicating nearby locations from the DBpedia dataset.Starting from this map, users can explore background informationabout locations and can navigate into interlinkeddatasets. DBpedia Mobile demonstrates that the DBpediadataset can serve as a useful starting point to explore theGeospatial Semantic Web using a mobile device.},
Url = {http://data.semanticweb.org/workshop/LDOW/2008/paper/6}
}
@inproceedings{
Title = {XSLT+SPARQL: Scripting the Semantic Web with SPARQL embedded into XSLT stylesheets},
Booktitle = {4th Workshop on Scripting for the Semantic Web (SFSW2008)},
Abstract = {Scripting the Semantic Web requires to access and transform RDF data. We present XSLT+SPARQL, a set of extension functions for XSLT which allow stylesheets to directly access RDF data, independently of any serialization syntax, by means of SPARQL queries. Using these functions, XSLT stylesheets can retrieve, query, merge and transform data from the semantic web. We illustrate the functionality of our proposal with an example script which creates XHTML pages from the contents of DBpedia.},
Url = {http://data.semanticweb.org/workshop/scripting/2008/paper/1}
}
@inproceedings{
Title = {Learning of OWL Class Descriptions on Very Large Knowledge Bases},
Booktitle = {7th International Semantic Web Conference (ISWC2008)},
Year = {2008},
Month = {October},
Abstract = {The vision of the Semantic Web is to make use of semantic representations on the largest possible scale - the Web. Large knowledge bases such as DBpedia, OpenCyc, GovTrack, and others are emerging and are freely available as Linked Data and SPARQL endpoints. Exploring and analysing such knowledge bases is a significant hurdle for Semantic Web research and practice. As one possible direction for tackling this problem, we present an approach for obtaining complex class descriptions from objects in knowledge bases by using Machine Learning techniques. We describe how we leverage existing techniques to achieve scalability on large knowledge bases available as SPARQL endpoints or Linked Data. Our algorithms are made available in the open source DL-Learner project and can be used in real-life scenarios by Semantic Web applications.},
Url = {http://data.semanticweb.org/conference/iswc/2008/paper/poster_demo/83}
}
@inproceedings{
Title = {Is There Anything Worth Finding on the Semantic Web?},
Booktitle = {18th International World Wide Web Conference (WWW2009)},
Year = {2009},
Month = {April},
Abstract = {There has recently been an upsurge of interest in the possibilities of combining structured data and ad-hoc information retrieval from traditional hypertext. In this experiment, we run queries extracted from a query log of a major search engine against the Semantic Web to discover if the Semantic Web has anything of interest to the average user. We show that there is indeed much information on the Semantic Web that could be relevant for many queries for people, places and even abstract concepts, although they are overwhelmingly clustered around a Semantic Web-enabled export of Wikipedia known as DBPedia. to a more specialized search engine. We use a search query log of approximately 15 million distinct queries from Microsoft Live Search. This query log contains 14,921,285 queries. Of these queries, 7,095,302 (47.55%) were unique, and corrected for capitalization, 6,623,635 (44.39%) were unique.},
Url = {http://data.semanticweb.org/conference/www/2009/paper/113}
}
@inproceedings{
Title = {Extracting Enterprise Vocabularies Using Linked Open Data},
Booktitle = {8th International Semantic Web Conference (ISWC2009)},
Year = {2009},
Month = {October},
Abstract = {A common vocabulary is vital to smooth business operation, yet codifying and maintaining an enterprise vocabulary is an arduous, manual task. We describe a process to automatically extract a domain specific vocabulary (terms and types) from unstructured data in the enterprise guided by term definitions in Linked Open Data (LOD). We validate our techniques by applying them to the IT (Information Technology) domain, taking 58 Gartner analyst reports and using two specific LOD sources -- DBpedia and Freebase.},
Url = {http://data.semanticweb.org/conference/iswc/2009/paper/inuse/143}
}
@inproceedings{
Title = {Enrichment and Ranking of the YouTube Tag Space and Integration with the Linked Data Cloud},
Booktitle = {8th International Semantic Web Conference (ISWC2009)},
Year = {2009},
Month = {October},
Abstract = {The increase of personal digital cameras with video functionality and video-enabled camera phones has increased the amount of user-generated videos on the Web. People are spending more and more time viewing online videos as a major source of entertainment and infotainment. Social websites allow users to assign shared free-form tags to user-generated multimedia resources, thus generating annotations for objects with a minimum amount of effort. Tagging allows communities to organize their multimedia items into browseable sets, but these tags may be poorly chosen and related tags may be omitted. Current techniques to retrieve, integrate and present this media to users are deficient and could do with improvement. In this paper we describe a framework for semantic enrichment, ranking and integration of web video tags using Semantic Web technologies. Semantic enrichment of folksonomies can bridge the gap between the uncontrolled and flat structures typically found in user-generated content and structures provided by the Semantic Web. The enhancement of tag spaces with semantics has been accomplished through two major tasks: (1) a tag space expansion and ranking step; and (2) through concept matching and integration with the Linked Data cloud. We have explored social, temporal and spatial contexts to enrich and extend the existing tag space. The resulting semantic tag space is modelled via a local graph based on co-occurrence distances for ranking. A ranked tag list is mapped and integrated with the Linked Data cloud through the DBpedia resource repository. Multi-dimensional context filtering for tag expansion means that tag ranking is much easier and it provides less ambiguous tag to concept matching.},
Url = {http://data.semanticweb.org/conference/iswc/2009/paper/inuse/171}
}
@inproceedings{
Title = {Learning Semantic Query Suggestions},
Booktitle = {8th International Semantic Web Conference (ISWC2009)},
Year = {2009},
Month = {October},
Abstract = {An important application of semantic web technology is recognizing human-defined concepts in text. Query transformation is a strategy often used in search engines to derive queries that are able to return more useful search results than the original query and most popular search engines provide facilities that let users complete, specify, or reformulate their queries. We study the problem of semantic query suggestion, a special type of query transformation based on identifying semantic concepts contained in user queries. We use a feature-based approach in conjunction with supervised machine learning, augmenting term-based features with search history-based and concept-specific features. We apply our method to the task of linking queries from real-world query logs (the transaction logs of the Netherlands Institute for Sound and Vision) to the DBpedia knowledge base. We evaluate the utility of different machine learning algorithms, features, and feature types in identifying semantic concepts using a manually developed test bed and show significant improvements over an already high baseline. The resources developed for this paper, i.e., queries, human assessments, and extracted features, are available for download.},
Url = {http://data.semanticweb.org/conference/iswc/2009/paper/research/501}
}
@inproceedings{
Title = {DSNotify: Handling Broken Links in the Web of Data},
Booktitle = {19th International World Wide Web Conference (WWW2010)},
Year = {2010},
Month = {April},
Abstract = {The Web of Data has emerged as a way of exposing structured linked data on the Web. It builds on the central building blocks of the Web (URIs, HTTP) and benefits from its simplicity and wide-spread adoption. It does, however, also inherit the unresolved issues such as the broken link problem. Broken links constitute a major issue for actors consuming Linked Data as they require them to deal with reduced accessibility of data. We believe that the broken link problem is a major threat to the whole Web of Data idea and that both Linked Data consumers and providers will require solutions that deal with this problem. Since no general solutions for fixing such links in the Web of Data have emerged, we make three contributions into this direction: first, we provide a concise definition of the broken link problem and a comprehensive analysis of existing approaches. Second, we present DSNotify, a generic framework able to assist human and machine actors in fixing broken links. It uses heuristic feature comparison and employs a time-interval-based blocking technique for the underlying instance matching problem. Third, we derived benchmark datasets from knowledge bases such as DBpedia and evaluate the effectiveness of our approach with respect to the broken link problem. Our results show the feasibility of a time-interval-based blocking approach for systems that aim at detecting and fixing broken links in the Web of Data.},
Url = {http://data.semanticweb.org/conference/www/2010/paper/main/324}
}
@inproceedings{
Title = {Interactive Relationship Discovery via the Semantic Web},
Booktitle = {7th Extended Semantic Web Conference (ESWC2010)},
Year = {2010},
Month = {June},
Abstract = {This paper presents an approach for the interactive discovery of relationships between selected elements via the Semantic Web. It fills the gap between algorithms that find relationships in datasets of the Semantic Web and their efficient usage in real-world contexts. Selected elements are first semi-automatically mapped to unique objects of Semantic Web datasets. These datasets are then crawled for relationships which are presented both, in detail and overview. Interactive features and visual clues allow for sophisticated exploration of the found relationships on different levels. The general process is described and the RelFinder tool as a concrete implementation and proof-of-concept is presented. The benefits and application potentials are illustrated by a scenario which uses the RelFinder and DBpedia to assist a business analyst in decision-making. Finally, the approach is evaluated in a user study, and discussed and compared with related work.},
Url = {http://data.semanticweb.org/conference/eswc/2010/paper/inuse/31}
}
@inproceedings{
Title = {Replication and Versioning of Partial RDF Graphs},
Booktitle = {7th Extended Semantic Web Conference (ESWC2010)},
Year = {2010},
Month = {June},
Abstract = {The sizes of datasets available as RDF (e.g., as part of the Linked Data cloud) are increasing continuously. For instance, the recent DBpedia version consists of nearly 500 millions triples. A common strategy to avoid problems that arise e.g., from limited network connectivity or lack of bandwidth is to replicate data locally, therefore making them accessible for applications without depending on a network connection. For mobile devices with limited capabilities, however, the replication and synchronization of billions of triples is not feasible. To overcome this problem, we propose an approach to replicate parts of an RDF graph to a client. Applications may then apply changes to this partial replica while being offline; these changes are written back to the original data source upon reconnection. Our approach does not require any kind of additional logic (e.g., change logging) or data structures on the client side, and hence is suitable to be applied on devices with limited computing power and storage capacity.},
Url = {http://data.semanticweb.org/conference/eswc/2010/paper/mobility/6}
}
@inproceedings{
Title = {Using social media for ontology enrichment},
Booktitle = {7th Extended Semantic Web Conference (ESWC2010)},
Year = {2010},
Month = {June},
Abstract = {In order to support informal learning, we complement the formal knowledge represented by ontologies developed by domain experts with the informal knowledge emerging from social tagging. To this end, we have developed an ontology enrichment pipeline that can automatically enrich a domain ontology using: data extracted by a crawler from social media applications, similarity measures, the DBpedia knowledge base, a disambiguation algorithm and several heuristics. The main goal is to provide dynamic and personalized domain ontologies that include the knowledge of the community of users. They will support a more personalized learning experience able to fulfill the needs of different types of learners.},
Url = {http://data.semanticweb.org/conference/eswc/2010/paper/social_web/29}
}
@inproceedings{
Title = {The Impact of Multifaceted Tagging on Learning Tag Relations and Search},
Booktitle = {7th Extended Semantic Web Conference (ESWC2010)},
Year = {2010},
Month = {June},
Abstract = {In this paper we present a model for multifaceted tagging, i.e. tagging enriched with contextual information. We present TagMe!, a social tagging front-end for Flickr images, that provides multifaceted tagging functionality: It enables users to attach tag assignments to a specific area within an image and to categorize tag assignments. Moreover, TagMe! automatically maps tags and categories to DBpedia URIs to clearly define the meaning of freely-chosen words. Our experiments reveal the benefits of those additional tagging facets. For example, the exploitation of those facets significantly improves the performance of FolkRank-based search. Further, we demonstrate the benefits of TagMe! tagging facets for learning semantics within folksonomies.},
Url = {http://data.semanticweb.org/conference/eswc/2010/paper/social_web/4}
}
@inproceedings{
Title = {Object Link Structure in the Semantic Web},
Booktitle = {7th Extended Semantic Web Conference (ESWC2010)},
Year = {2010},
Month = {June},
Abstract = {Lots of RDF data have been published in the Semantic Web. The RDF data model, together with the decentralized linkage nature of the Semantic Web, brings object link structure to the worldwide scope. Object links are critical to the Semantic Web and the macroscopic properties of object links are helpful for better understanding the current Data Web. In this paper, we propose a notion of object link graph (OLG) in the Semantic Web, and analyze the complex network structure of an OLG constructed from the latest dataset (FC09) collected by Falcons. We find that the OLG has the scale-free nature and the effective diameter of the graph is small compared to its scale. By another experimental result on the last year''s dataset (FC08), we confirm our findings and observe that the object link graph is becoming denser and its diameter is shrinking during the past year, which indicates a good evolution of the Data Web. Finally, we repeat the complex network analysis on the two largest domain-specific subsets of FC09, namely Bio2RDF(FC09) and DBpedia(FC09). The results show that both Bio2RDF(FC09) and DBpedia(FC09) have low density in object links, which has great influence on the density of object links in FC09.},
Url = {http://data.semanticweb.org/conference/eswc/2010/paper/web_of_data/8}
}
@inproceedings{
Author = {Haizhou Fu and Sidan Gao and Kemafor Anyanwu},
Title = {CoSi: Context-Sensitive Keyword Query Interpretation on RDF Databases},
Booktitle = {20th International World Wide Web Conference (WWW2011)},
Year = {2011},
Month = {March/April},
Pages = {209},
Abstract = {The demo will present CoSi, a system that enables context-sensitive interpretation of keyword queries on RDF databases. The techniques for representing, managing and exploiting query history are central to achieving this objective. The demonstration will show the effectiveness of our approach for capturing a user's querying context from their query history. Further, it will show how context is utilized to influence the interpretation of a new query. The demonstration is based on DBPedia, the RDF representation of Wikipedia.},
Url = {http://data.semanticweb.org/conference/www/2011/demo/cosi-context-sensitive-keyword-query-interpretatio}
}
@inproceedings{
Booktitle = { ()},
Url = {http://data.semanticweb.org/conference/www/2011/demo/einstein-physicist-or-vegetarian}
}
@inproceedings{
Author = {Jens Lehmann and Lorenz Bühmann},
Title = {AutoSPARQL: Let Users Query Your Knowledge Base},
Booktitle = {8th Extended Semantic Web Conference (ESWC2011)},
Year = {2011},
Month = {June},
Abstract = {An advantage of Semantic Web standards like RDF and OWL is their flexibility in modifying the structure of a knowledge base. To turn this flexibility into a practical advantage, it is of high importance to have tools and methods, which offer similar flexibility in extracting information from a knowledge base. This is closely related to the ability to easily formulate queries over those knowledge bases. We explain benefits and drawbacks of existing techniques in achieving this goal and then present the QTL algorithm, which fills a gap in research and practice. It uses supervised machine learning and allows users to ask queries without knowing the schema of the underlying knowledge base beforehand and without expertise in the SPARQL query language. We then present the AutoSPARQL user interface, which implements an active learning approach on top of QTL. Finally, we present an evaluation based on the SPARQL query log of the DBpedia knowledge base.},
Url = {http://data.semanticweb.org/conference/eswc/2011/paper/inductive-approaches/6}
}
@inproceedings{
Author = {Johanna Voelker and Mathias Niepert},
Title = {Statistical Schema Induction},
Booktitle = {8th Extended Semantic Web Conference (ESWC2011)},
Year = {2011},
Month = {June},
Abstract = {While the realization of the Semantic Web as once envisioned by Tim Berners-Lee remains in a distant future, the Web of Data has already become a reality. Billions of RDF statements out there on the Internet, facts about a variety of different domains, are ready to be used by semantic applications. Some of these applications, however, crucially hinge on the availability of expressive schemas suitable for logical inference that yields non-trivial conclusions. In this paper, we present a statistical approach to the induction of expressive schemas from large RDF repositories. We describe in detail the implementation of this approach and report on an evaluation that we conducted using several data sets including DBpedia.},
Url = {http://data.semanticweb.org/conference/eswc/2011/paper/linked-open-data/8}
}
@inproceedings{
Booktitle = { ()},
Url = {http://data.semanticweb.org/conference/eswc/2011/paper/services/5}
}
@inproceedings{
Title = {DC Proposal: Ontology Learning from Noisy Linked Data},
Booktitle = {10th International Semantic Web Conference (ISWC2011)},
Abstract = {Mobile devices like smartphones together with social networks enable people to generate, share, and consume enormous amounts of media content. Common search operations, for example searching for a music clip based on artist name and song title on video platforms such as YouTube, can be achieved both based on potentially shallow human-generated metadata, or based on more profound content analysis, driven by Optical Character Recognition (OCR) or Automatic Speech Recognition (ASR). However, more advanced use cases, such as summaries or compilations of several pieces of media content covering a certain event, are hard, if not impossible to fulfill at large scale. One example of such event can be a keynote speech held at a conference, where, given a stable network connection, media content is published on social networks while the event is still going on.In our thesis, we develop a framework for media content processing, leveraging social networks, utilizing the Web of Data and fine-grained media content addressing schemes like Media Fragments URIs to provide a scalable and sophisticated solution to realize the above use cases: media content summaries and compilations. We evaluate our approach on the entity level against social media platform APIs in conjunction with Linked (Open) Data sources, comparing the current manual approaches against our semi-automated approach. Our proposed framework can be used as an extension for existing video platforms.},
Url = {http://data.semanticweb.org/conference/iswc/2011/paper/doctoral-consortium/20}
}
@inproceedings{
Title = {NERD: A Framework for Evaluating Named Entity Recognition Tools in the Web of Data},
Booktitle = {10th International Semantic Web Conference (ISWC2011)},
Abstract = {In this paper, we present NERD, an evaluation framework we have developed that records and analyzes ratings of Named Entity (NE) extraction and disambiguation tools working on English plain text articles performed by human beings. NERD enables the comparison of different popular Linked Data entity extractors which expose APIs such as AlchemyAPI, DBPedia Spotlight, Extractiv, OpenCalais and Zemanta. Given an article and a particular tool, a user can assess the precision of the named entities extracted, their typing and linked data URI provided for disambiguation and their subjective relevance for the text. All user interactions are stored in a database. We propose the NERD ontology that deﬁnes mappings between the types detected by the diﬀerent NE extractors. The NERD framework enables then to visualize the comparative performance of these tools with respect to human assessment. Key words: Entity extraction, Linked Data, Natural Language Processing, Evaluation of Linked Data entity extraction tools 1},
Url = {http://data.semanticweb.org/conference/iswc/2011/paper/poster-demo/35}
}
@inproceedings{
Title = {DBpedia internationalization - a graphical tool for I18n infobox-to-ontology mappings},
Booktitle = {10th International Semantic Web Conference (ISWC2011)},
Abstract = {During the past two decades, the use of the Web has spread across multiple countries and cultures. While the Semantic Web is already served in many languages, we are still facing challenges concerning its internationalization. The DBpedia project, a community eﬀort to extract structured information from Wikipedia, is already supporting multiple languages. This paper presents a graphical tool for creating internationalized mappings for DBpedia.},
Url = {http://data.semanticweb.org/conference/iswc/2011/paper/poster-demo/36}
}
@inproceedings{
Title = {TOPICA: A Tool for Visualising Emerging Semantics of POIs based on Social Awareness Streams},
Booktitle = {10th International Semantic Web Conference (ISWC2011)},
Abstract = {<i>Topica </i>is an application that enriches the Social Web with semantic data, to enable collective perception of Points of Interest (POIs), which are human constructs that describe information about locations (e.g., restaurants, attractions, cities). <i>Topica </i>provides an extra layer of information, compared to existing applications for browsing POIs, by modelling hidden characteristics of POIs, by: (1) generating a Linked Data representation of the collective perception of a POI; (2) enhancing the POI representation by mashing up services that enrich the POI’s related entities; and (3) providing a visual representation of the POIs adapted to suit userand context-sensitive ﬁlters. <i>Topica </i>identiﬁes topics relevant to a POI by extracting DBpedia categories from <i>entities </i>(e.g., People, Places) and <i>keywords</i> (e.g., Crete, Bonn) obtained from social awareness streams related to the POIs. <b>Key words: </b>Linked Data, Semantic Web, Point of Interest, Social Awareness Streams, citizen-sensing, social data mining, emerging semantics <b>1</b>},
Url = {http://data.semanticweb.org/conference/iswc/2011/paper/poster-demo/70}
}
@inproceedings{
Title = {Effectively Interpreting Keyword Queries on RDF Databases with a Rear View},
Booktitle = {10th International Semantic Web Conference (ISWC2011)},
Abstract = {Eﬀective techniques for keyword search over RDF databases incorporate an explicit interpretation phase that maps keywords in a keyword query to structured query constructs. Because of the ambiguity of keyword queries, it is often not possible to generate a unique interpretation for a keyword query. Consequently, heuristics geared toward generating the top-K likeliest user-intended interpretations have been proposed. However, heuristics currently proposed fail to capture any userdependent characteristics, but rather depend on database-dependent properties such as occurrence frequency of subgraph pattern connecting keywords. This leads to the problem of generating top-K interpretations that are not aligned with user intentions. In this paper, we propose a contextaware approach for keyword query interpretation that personalizes the interpretation process based on a user’s query context. Our approach addresses the novel problem of using a sequence of structured queries corresponding to interpretations of keyword queries in the query history as contextual information for biasing the interpretation of a new query. Experimental results presented over DBPedia dataset show that our approach outperforms the state-of-the-art technique on both eﬃciency and eﬀectiveness, particularly for ambiguous queries.},
Url = {http://data.semanticweb.org/conference/iswc/2011/paper/research/325}
}
@inproceedings{
Title = {DBpedia SPARQL Benchmark – Performance Assessment with Real Queries on Real Data},
Booktitle = {10th International Semantic Web Conference (ISWC2011)},
Abstract = {Triple stores are the backbone of increasingly many Data Web applications. It is thus evident that the performance of those stores is mission critical for individual projects as well as for data integration on the Data Web in general. Consequently, it is of central importance during the implementation of any of these applications to have a clear picture of the weaknesses and strengths of current triple store implementations. In this paper, we propose a generic SPARQL benchmark creation procedure, which we apply to the DBpedia knowledge base. Previous approaches often compared relational and triple stores and, thus, settled on measuring performance against a relational database which had been converted to RDF by using SQL-like queries. In contrast to those approaches, our benchmark is based on queries that were actually issued by humans and applications against existing RDF data not resembling a relational schema. Our generic procedure for benchmark creation is based on query-log mining, clustering and SPARQL feature analysis. We argue that a pure SPARQL benchmark is more useful to compare existing triple stores and provide results for the popular triple store implementations Virtuoso, Sesame, Jena-TDB, and BigOWLIM. The subsequent comparison of our results with other benchmark results indicates that the performance of triple stores is by far less homogeneous than suggested by previous benchmarks. 1},
Url = {http://data.semanticweb.org/conference/iswc/2011/paper/research/42}
}
@inproceedings{
Title = {Leveraging Community-built Knowledge for Type Coercion in
Question Answering},
Booktitle = {10th International Semantic Web Conference (ISWC2011)},
Abstract = {Error},
Url = {http://data.semanticweb.org/conference/iswc/2011/paper/semantic-web-in-use/18}
}
@inproceedings{
Title = {Zhishi.me - Weaving Chinese Linking Open Data},
Booktitle = {10th International Semantic Web Conference (ISWC2011)},
Abstract = {Linking Open Data (LOD) has become one of the most
important community
efforts to publish high-quality interconnected semantic
data. Such data has
been widely used in many applications to provide
intelligent services like
entity search, personalized recommendation and so on. While
DBpedia, one of
the LOD core data sources, contains resources described in
multilingual
versions and semantic data in English is proliferating,
there is very few
work on publishing Chinese semantic data. In this paper, we
present
Zhishi.me, the first effort to publish large scale Chinese
semantic data and
link them together as a Chinese LOD (CLOD). More precisely,
we identify
important structural features in three largest Chinese
encyclopedia sites
(i.e., Baidu Baike, Hudong Baike, and Chinese Wikipedia)
for extraction and
propose several data-level mapping strategies for automatic
link discovery.
As a result, the CLOD has more than 5 million distinct
entities and we simply
link CLOD with the existing LOD based on the multilingual
characteristic of
Wikipedia. Finally, we also introduce three Web access
entries namely SPARQL
endpoint, lookup interface and detailed data view, which
conform to the
principles of publishing data sources to LOD.},
Url = {http://data.semanticweb.org/conference/iswc/2011/paper/semantic-web-in-use/70}
}
@inproceedings{
Title = {Semantic Web Technology in Watson},
Booktitle = {10th International Semantic Web Conference (ISWC2011)},
Abstract = {Watson (http://ibmwatson.com) is an open-domain natural language question answering system that answers questions with precision and confidence rivaling the best human experts at the task, and that emerged victorious in a widely viewed public challenge on the American TV Quiz Show, Jeopardy!.  Semantic Web Technology, enhanced by a massive use of open linked data, plays a crucial role in Watson. For example, linked data such as DBpedia and Geonames and triple stores such as Sesame have been used to generate candidate answers and to score them under multiple points of view such as type coercion and geographic proximity.  In addition, the connection between linked data and natural language text offered by Wikipedia has been very useful to generate open domain training data for relation detection and entity recognition systems, substantially improving the NLP capabilities of the system and therefore allowing the development of a truly open domain QA system. This tutorial is focus on the Semantic Web Technology adopted by Watson and on how it fits in the general Deep QA architecture. The tutorial is structured in two parts, the first focusing on the open-domain QA challenge and the Watson architecture, the second focused on the use of semantic web technology and linked open data in Watson.  Tutorial webpage: http://iswc2011.semanticweb.org/tutorials/semantic-web-technology-in-watson/},
Url = {http://data.semanticweb.org/conference/iswc/2011/paper/tutorial/11}
}
@inproceedings{
Title = {Statistical Analysis of Web of Data Usage},
Booktitle = {Joint Workshop on Knowledge Evolution and Ontology Dynamics (EvoDyn2011)},
Abstract = {The Linked Data initiative gained momentum inside as well as outside of theresearch community. Thus, it is already an accepted research issue to investigate usage mining in the context ofthe Web of Data from various perspectives. We are currently working onan approach that applies such usage mining methods and analysis to support ontology and datasetmaintenance tasks. This paper presents one part of this work, namely a methodto detect errors or weaknesses within ontologies used for Linked Data populationbased on statistics and network visualizations. We contribute a detailed description of a log file preprocessing algorithm for Web of Data endpoints, a set of statistical measures that help to visualize different usage aspects, and an examplary analysis of one of the most prominent Linked Data set -- DBpedia -- aimed to show the feasibility and the potential of our approach.},
Url = {http://data.semanticweb.org/workshop/evodyn/2011/paper/1}
}
@inproceedings{
Title = {Linking Domain-Specific Knowledge to Encyclopedic Knowledge: an Initial Approach to Linked Data},
Booktitle = {2nd Workshop on the Multilingual Semantic Web (MSW2011)},
Abstract = {Linked Data creates a shared information space by publishing andconnecting resources in the Semantic Web. However, the specification ofsemantic relationships between data sources is still a stumbling block. Onesolution is to enrich ontologies with multilingual and concept-orientedinformation. Usefully linking entities in the Semantic Web is thus facilitated bya semantic-oriented cross-lingual ontology mapping framework in whichknowledge representations are not restricted to a particular natural language.Accordingly, this paper describes a preliminary approach for integrating generalencyclopedic knowledge in DBpedia with EcoLexicon, a multilingualterminological knowledge base on the environment.},
Url = {http://data.semanticweb.org/workshop/msw/2011/paper/3}
}
@inproceedings{
Title = {The Quad Economy of a Semantic Web Ontology Repository},
Booktitle = {The 7th International Workshop on Scalable Semantic Web Knowledge Base Systems (SSWS2011)},
Abstract = {Publishers of Linked Data require scalable storage and retrieval in- frastructure due to the size of datasets and potentially high rate of lookups on popular sites. In this paper we investigate the feasibility of using a distributed key-value store as an underlying storage component for a Linked Data server which provides functionality for serving Linked Data via HTTP lookups and in addition offers single triple pattern lookups. We devise two storage schemes for our CumulusRDF system implemented on Apache Cassandra, an open-source key-value store. We compare the schemes with a state-of-the-art distributed RDF store on a subset of DBpedia and both synthetic workloads and workloads ob- tained from DBpedia’s access logs. Results on a cluster of up to 8 machines show that CumulusRDF is competetive to state-of-the-art distributed RDF stores.},
Url = {http://data.semanticweb.org/workshop/ssws/2011/paper/10}
}
@inproceedings{
Author = {Mohamed Yahya and Klaus Berberich and Shady Elbassuoni and Maya Ramanath and Volker Tresp and Gerhard Weikum},
Title = {Deep Answers for Naturally Asked Questions on the Web of Data},
Booktitle = {21st International World Wide Web Conference (WWW2012)},
Year = {2012},
Abstract = {We present DEANNA, a framework for natural language question answering over structured knowledge bases. Given a natural language question, DEANNA translates questions into a SPARQL-like structured query language that can be evaluated over knowledge bases such as Yago, Dbpedia, Freebase, or other Linked Data sources.DEANNA analyzes questions and aims to map verbal phrases to relations and noun phrases to either individual entities or semantic classes. Importantly, it judiciously generates variables for target entities or classes to express joins between multiple triple patterns.In contrast to prior work on QA, we leverage the semantic type system for entities and we use constraints in jointly mapping the constituents of the question to relations, classes, and entities. We demonstrate the capabilities and interface of DEANNA, which allows advanced users to influence the translation process and view how the different components interact to produce the final result.},
Url = {http://data.semanticweb.org/conference/www/2012/demo/41}
}
@inproceedings{
Author = {Maurizio Atzori and Carlo Zaniolo},
Title = {SWiPE: Searching Wikipedia By Example},
Booktitle = {21st International World Wide Web Conference (WWW2012)},
Year = {2012},
Abstract = {A novel method is demonstrated that allows semantic and well-structured knowledge bases (such as DBpedia) to be easily queried directly from Wikipedia’s pages. Using Swipe, naive users with no knowledge of RDF triples and sparql can easily query DBpedia with powerful questions such as: “Who are the U.S. presidents who took office when they were 55-year old or younger, during the last 60 years”, or “Find the town in California with less than 10 thousand people”. This is accomplished by a novel Search by Example (SBE) approach where a user can enter the query conditions directly on the Infobox of a Wikipedia page. In fact, Swipe activates various fields of Wikipedia to allow users to enter query conditions, and then uses these conditions to generate equivalent sparql queries and execute them on DBpedia. Finally, Swipe returns the query results in a form that is conducive to query refinements and further explorations. Swipe’s SBE approach makes semistructured documents queryable in an intuitive and user-friendly way and, through Wikipedia, delivers the benefits of querying and exploring large knowledge bases to all Web users.},
Url = {http://data.semanticweb.org/conference/www/2012/demo/53}
}
@inproceedings{
Author = {Thomas Kurz and Sebastian Schaffert and Manuel Fernandez and Georg Güntner},
Title = {Adding Wings to Red Bull Media: Search and Display semantically enhanced Video Fragments},
Booktitle = {21st International World Wide Web Conference (WWW2012)},
Year = {2012},
Abstract = {The Linked Data movement with the aims of publishing and interconnecting machine readable data has originated in the last decade. Although the set of (open) data sources is rapidly growing, the integration of multimedia in this Web of Data is still at a very early stage. This paper describes, how arbitrary video content and metadata can be processed to identify meaningful linking partners for video fragments - and thus create a web of linked media. The video test-set for our demonstrator is part of the Red Bull Content Pool and confined to the Cliff Diving domain. The candidate set of possible link targets is a combination of a Red Bull thesaurus, information about divers from www.redbull.com and concepts from DBPedia. The demo includes both a semantic search on videos and video fragments and a player for videos with semantic enhancements.},
Url = {http://data.semanticweb.org/conference/www/2012/demo/65}
}
@inproceedings{
Author = {Yves Raimond and Chris Lowis and Jonathan Tweed and Roderick Hodgson},
Title = {Automated semantic tagging of speech audio},
Booktitle = {21st International World Wide Web Conference (WWW2012)},
Year = {2012},
Abstract = {The BBC is currently tagging programmes manually, using DBpedia as a source of tag identifiers, and a list of sug- gested tags extracted from their synopsis. These tags are then used to help navigation and topic-based search of BBC programmes. However, given the very large number of pro- grammes available in the archive, most of them having very little metadata attached to them, we need a way of automat- ically assigning tags to programmes. We describe a frame- work to do so, using speech recognition, text processing and concept tagging techniques.
We evaluate this framework against manually applied tags, and compare it with related work. We find that this frame- work has better performances than related work for this task, and is good enough to bootstrap the tagging process of archived content. We describe Tellytopic, an application us- ing automatically extracted tags to aid discovery of archive content.},
Url = {http://data.semanticweb.org/conference/www/2012/demo/87}
}
@inproceedings{
Author = {Olivier Grisel},
Title = {Automated Linking Data with Apache Stanbol},
Booktitle = {21st International World Wide Web Conference (WWW2012)},
Year = {2012},
Abstract = {This talk will introduce the Stanbol project and showcase how it can be integrated in traditional Enterprise Content Management products.Stanbol is an Open Source project under incubation at the Apache Software Foundation. Its goal is to provide Web and CMS developers with a set of HTTP / RESTful services to help them integrate semantic technologies into their products and web sites.The following Stanbol services are currently under active developments:- Enhancement engines: use Natural Language Processing tools such as Apache OpenNLP or external services to extract knowledge (topics, named entities, facts) from unstructured content and link it to unambiguous URIs from reference knowledge bases;- Entity Hub: a Linked Data indexing cache built on top of Apache Solr, Clerezza and Jena that comes with precomputed indexes and live connectors to popular knowledge bases such as DBpedia, Geonames, YAGO or custom SKOS taxonomies...- Content Hub: a faceted search engine based on Solr to search for content using the knowledge automatically extracted by the enhancement engines;- CMS bridges to lift the structured content of document repositories using the JCR and CMIS access protocols (using Apache Chemistry) and store the result into a triple store suitable for SPARQL access;- Rules engine based on Apache Jena for knowledge refactoring (e.g. convert extracted knowledge into the rich snippet vocabulary for SEO), integrity checks, merging rules, deductive inference...The Semantic Web has made significant progress over the last years, and while it always gave a lot of promises, it is now the time where it can concretely be used in Enterprise Solutions.If you are curious about the web of data, and want to see how concretely it can be used and integrated today in enterprise solutions thanks to software like the Stanbol projects, this session is for you.Stanbol project homepage: https://incubator.apache.org/stanbol/Full stanbol demo of Stanbol Services: http://dev.iks-project.eu:8081/## Note for the reviewers (to be removed from the published abstract):Here is the slide deck of a similar talk I gave a ApacheCon in November 2011. http://www.slideshare.net/nuxeo/apache-stanbol-and-the-web-of-data-apachecon-2011You can also connect to this Nuxeo Document Management demo that uses Stanbol:http://temis.demo.nuxeo.comLogin: AdministratorPassword: AdministratorTo test the Nuxeo / Stanbol / Temis / DBpedia integration you can go to a workspace an create a new document of type "Note" and copy and paste text from some wikinews article. Upon saving the content will automatically get analyzed and linked to referenced entities.Note: this demo has not been updated for a while. For wwww2012 I will also be able to demonstrate automated topic classification (rather that just entities occurences).},
Url = {http://data.semanticweb.org/conference/www/2012/dev/42}
}
@inproceedings{
Author = {Sasidhar Kasturi and Soumen Chakrabarti and Ganesh Ramakrishnan and Bharath Balakrishnan and Rohit Saraf},
Title = {Compressed Data Structures for Annotated Web Search},
Booktitle = {21st International World Wide Web Conference (WWW2012)},
Year = {2012},
Month = {April},
Abstract = {Entity relationship search at Web scale depends on adding dozens of entity annotations to each of billions of crawled pages and indexing the annotations at rates comparable to regular text indexing.  Even small entity search benchmarks from TREC and INEX suggest that the entity catalog support thousands of entity types and tens to hundreds of millions of entities.  The above targets raise many challenges, major ones being the design of highly compressed data structures in RAM for spotting and disambiguating entity mentions, and highly compressed disk-based annotation indices.  These data structures cannot be readily built upon standard inverted indices.  Here we present the fastest known Web scale entity annotator.  Using a new workload-sensitive compressed multilevel map, we fit statistical disambiguation models for millions of entities within 1.1GB of RAM, and spend about 0.6 core-milliseconds per disambiguation.  In contrast, DBPedia Spotlight spends 158 milliseconds, Wikipedia Miner spends 21 milliseconds, and Zemanta spends 9.5 milliseconds.  Our annotation indices use ideas from vertical databases to reduce storage by 30%.  On 40x8 cores with 40x3 disk spindles, we can annotate and index a billion Web pages with Wikipedia's two million entities and over 200,000 types in about a day.  Index decompression and scan speed is comparable to MG4J.},
Url = {http://data.semanticweb.org/conference/www/2012/paper/773}
}
@inproceedings{
Author = {Yves Raimond and Chris Lowis},
Title = {Automated interlinking of speech radio archives},
Booktitle = {LDOW 2012 : linked Data on the web (LDOW2012)},
Year = {2012},
Abstract = {The BBC is currently tagging programmes manually, using DBpedia as a source of tag identifiers, and a list of suggested tags extracted from their synopsis. These tags are then used to help navigation and topic-based search of BBC programmes. However, given the very large number of programmes available in the archive, most of them having very little metadata attached to them, we need a way of automatically assigning tags to programmes. We describe a framework to do so, using speech recognition, text processing and concept tagging techniques. We evaluate this framework against manually applied tags, and compare it with related work. We find that this framework is good enough to bootstrap the interlinking process of archived content.},
Url = {http://data.semanticweb.org/workshop/ldow/2012/paper/15}
}
@inproceedings{
Author = {Luis Daniel Ibáñez and Hala Skaf-Molli and Pascal Molli and Olivier Corby},
Title = {Synchronizing semantic stores with Commutative Replicated Data Types},
Booktitle = {SWCS: Semantic Web Collaborative Spaces (SWCS 2012)},
Year = {2012},
Abstract = {Social semantic web technologies led to huge amounts of data and information being available. The production of knowledge from this information is challenging, and ma- jor efforts, like DBpedia, has been done to make it reality. Linked data provides interconnection between this informa- tion, extending the scope of the knowledge production.
The knowledge construction between decentralized sources in the web follows a co-evolution scheme, where knowledge’s generated collaboratively and continuously. Sources are also autonomous, meaning that they can use and publish only the information they want.
The updating of sources with this criteria is intimately re- lated with the problem of synchronization, and the consis- tency between all the replicas managed.
Recently, a new family of algorithms called Commutative Replicated Data Types have emerged for ensuring eventual consistency in highly dynamic environments. In this paper, we define SU-Set, a CRDT for RDF-Graph that supports SPARQL Update 1.1 operations.},
Url = {http://data.semanticweb.org/workshop/swcs/2012/paper/13}
}
@inproceedings{
Author = {Heiko Paulheim},
Title = {Nobody Wants to Live in a Cold City where no Music Has Been Recorded - Analyzing Statistics with Explain-a-LOD},
Booktitle = {9th Extended Semantic Web Conference (ESWC2012)},
Year = {2012},
Month = {May},
Abstract = {While it is easy to find statistics on almost every topics, coming up with an explanation about those statistics is a much more difficult task. This demo showcases the prototype tool Explain-a-LOD, which uses background knowledge from DBpedia for generating possible explanations for a statistic.},
Url = {http://data.semanticweb.org/conference/eswc/2012/paper/demonstation/315}
}
@inproceedings{
Author = {Xueyan Jiang and Yi Huang and Maximilian Nickel and Volker Tresp},
Title = {Exploiting  Information Extraction,  Reasoning and Machine Learning for Relation Prediction},
Booktitle = {9th Extended Semantic Web Conference (ESWC2012)},
Year = {2012},
Month = {May},
Abstract = {The three most common approaches for deriving or predicting   instantiated  relations, i.e.  triple  statements (s, p, o), are information extraction, reasoning and relational machine learning.  Information extraction uses sensory information, typically in form of text, and extracts statements using various methods ranging from  simple classifiers to the most  sophisticated NLP approaches. Logical reasoning is based on a set of true statements and derives new statements via inference using higher-order logical axioms. Finally, machine learning exploits regularities in the data to predict the likelihood of new statements. In this paper we combine all three  methods to exploit all sources of available information in a modular way, by which we mean that each approach, i.e., information extraction, reasoning, machine learning,  can be optimized independently to be  combined in an overall system. For relational machine learning, we present a novel approach based on hierarchical Bayesian multi-label learning which also sheds new light on common factorization approaches. We rank the probabilities for statements to be true in the sense that: given that we are forced to make a decision, what is the best option. We consider the fact that an entity can belong to more than one ontological class and discuss aggregation. We extend the approach to modeling nonlinear dependencies between  relationships and  for personalization. We validate our model using data from the Yago and the DBpedia ontology.},
Url = {http://data.semanticweb.org/conference/eswc/2012/paper/research/171}
}
@inproceedings{
Author = {Isabelle Augenstein and Sebastian Pado and Sebastian Rudolph},
Title = {LODifier: Generating Linked Data from Unstructured Text},
Booktitle = {9th Extended Semantic Web Conference (ESWC2012)},
Year = {2012},
Month = {May},
Abstract = {The automated extraction of information from text and its transformation into a formal description is an important goal of in both Semantic Web research and computational linguistics. The extracted information can be used for a variety of tasks such as ontology generation, question answering and information retrieval. LODifier is an approach that combines deep semantic analysis with named entity recognition, word-sense disambiguation and controlled Semantic Web vocabularies in order to extract named entities and relations between them from text and to convert them into an RDF representation which is linked to DBpedia and WordNet. We present the architecture of our tool and discuss design decisions made. Evaluations of the tool give clear evidence of its potential for tasks like information extraction and computing document similarity.},
Url = {http://data.semanticweb.org/conference/eswc/2012/paper/research/73}
}
